# RLang

## Метрические алгоритмы

### 1. [Метод ближайших соседей (k closest neigbors)](https://github.com/Dembele/RLang/tree/master/knn)
Простейший метрический классификатор, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.
Метод ближайшего соседа является, пожалуй, самым простым алгоритмом классификации. Классифицируемый объект x относится к тому классу y_i, которому принадлежит ближайший объект обучающей выборки x_i.
Метод k ближайших соседей. Для повышения надёжности классификации объект относится к тому классу, которому принадлежит большинство из его соседей — k ближайших к нему объектов обучающей выборки x_i. В задачах с двумя классами число соседей берут нечётным, чтобы не возникало ситуаций неоднозначности, когда одинаковое число соседей принадлежат разным классам.
![alt text](https://github.com/Dembele/RLang/blob/master/knn/knn.png?raw=true "Карта классификации для 1 соседа")
Карта классификации для 1 соседа цветов ириса по длине и ширине лепестков.

### 2.  [Метод Парзеновского окна](https://github.com/Dembele/RLang/tree/master/pw)
Метод байесовской классификации, основанный на непараметрическом восстановлении плотности по имеющейся выборке.
После ввода метрики, метод парзеновского окна можно использовать, не опираясь на вероятностную природу данных.
В основе подхода лежит идея о том, что плотность выше в тех точках, рядом с которыми находится большое количество объектов выборки.
![alt text](https://github.com/Dembele/RLang/blob/master/pw/loo.pw.png?raw=true "Зависимость количества ошибок от величины окна")

### 3. [Метод потенциальных функций](https://github.com/Dembele/RLang/tree/master/pf)
метрический классификатор, частный случай метода ближайших соседей. Позволяет с помощью простого алгоритма оценивать вес («важность») объектов обучающей выборки при решении задачи классификации.
Общая идея метода иллюстрируется на примере электростатического взаимодействия элементарных частиц. Известно, что потенциал («мера воздействия») электрического поля элементарной заряженной частицы в некоторой точке пространства пропорционален отношению заряда частицы к расстоянию до частицы
Метод потенциальных функций реализует полную аналогию указанного выше примера. При классификации объект проверяется на близость к объектам из обучающей выборки. Считается, что объекты из обучающей выборки «заряжены» своим классом, а мера «важности» каждого из них при классификации зависит от его «заряда» и расстояния до классифицируемого объекта.

### 4. [Алгоритм СТОЛП](https://github.com/Dembele/RLang/tree/master/stolp)
алгоритм отбора эталонных объектов для метрического классификатора.
Эталоны — это такое подмножество выборки X^l, что все объекты X^l (или их большая часть) классифицируются правильно при использовании в качестве обучающей выборки множества эталонов.
Эталонами i-го класса при классификации методом ближайшего соседа может служить такое подмножество объектов этого класса, что расстояние от любого принадлежащего ему объекта из выборки X^l до ближайшего «своего» эталона меньше, чем до ближайшего «чужого» эталона.
Простой перебор для отбора эталонов не эффективен. Алгоритм STOLP позволяет сократить этот перебор

### 5. Скользящий контроль/кросс-проверка/кросс-валидация (cross-validation, CV)
Скользящий контроль или кросс-проверка или кросс-валидация (cross-validation, CV) — процедура эмпирического оценивания обобщающей способности алгоритмов, обучаемых по прецедентам.
Фиксируется некоторое множество разбиений исходной выборки на две подвыборки: обучающую и контрольную. Для каждого разбиения выполняется настройка алгоритма по обучающей подвыборке, затем оценивается его средняя ошибка на объектах контрольной подвыборки. Оценкой скользящего контроля называется средняя по всем разбиениям величина ошибки на контрольных подвыборках.
Если выборка независима, то средняя ошибка скользящего контроля даёт несмещённую оценку вероятности ошибки. Это выгодно отличает её от средней ошибки на обучающей выборке, которая может оказаться смещённой (оптимистически заниженной) оценкой вероятности ошибки, что связано с явлением переобучения.
Скользящий контроль является стандартной методикой тестирования и сравнения алгоритмов классификации, регрессии и прогнозирования.
![alt text](https://github.com/Dembele/RLang/blob/master/knn/loo.knn.png?raw=true "loo для knn")